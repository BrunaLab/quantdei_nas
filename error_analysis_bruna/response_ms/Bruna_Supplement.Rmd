---
title             : "Supplement to 'Fundamental errors of data collection & validation undermine claims of 'Ideological Intensification' made by the National Association of Scholars'"
shorttitle        : "Flawed data validation by the NAS"

author: 
  - name          : "Emilio M. Bruna"
    affiliation   : "1,2"
#    address       : "Department of Wildlife Ecology and Conservation, University of Florida, PO Box 110430, Gainesville, FL 32611-0430, USA"
    email         : "embruna@ufl.edu"
    # role:
    #   - Methodology
    #   - Data curation
    #   - Investigation
    #   - Funding acquisition
    #   - Conceptualization
    #   - Formal analysis
    #   - Methodology
    #   - Project administration
    #   - Resources
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Visualization
    #   - Writing original draft

affiliation:
  - id            : "1"
    institution   : "Department of Wildlife Ecology and Conservation, University of Florida, PO Box 110430, Gainesville, FL 32611-0430, USA"
  - id            : "2"
    institution   : "Center for Latin American Studies, University of Florida, PO Box 115530, Gainesville, FL 32611-5530, USA"

authornote: All code and data used in this analysis are available at https://github.com/embruna/quantdei_nas. 

# 
# abstract: |
#   Abstract of the paper.
  
  
  
keywords          : "Up,to,eight,keywords"
wordcount         : "X"

bibliography      : "NAS_response.bib"

floatsintext      : no
figurelist        : yes
tablelist         : no
footnotelist      : no
numbersections    : no
linenumbers       : yes
mask              : no
draft             : no
# replace_ampersands: yes

classoption       : "man, donotrepeattitle" # suppresses title on 1st page of MS

mainfont: Times New Roman
fontsize: 12pt
csl: "science.csl"
fig_caption: yes
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot{} # remove page number bottom of the page
- \fancyhead[L]{E. M. Bruna, (Supplement)}
- \fancyhead[R]{p. \thepage}
- \usepackage{setspace}
- \usepackage{parskip} 
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \usepackage{caption}                            
- \DeclareCaptionLabelFormat{Sformat}{#1 S#2}     
- \captionsetup[table]{labelformat=Sformat}  
- \captionsetup[figure]{labelformat=Sformat}  
keep_tex: true
output: papaja::apa6_pdf
      
# output: pdf_document
    # keep_tex: true
# 
# header-includes:
#   - \raggedbottom
#   - \usepackage{endfloat} #[nomarkers] excludes the {insert figure x around here] from main text. The others exclude the list of tables and figures. https://cs.brown.edu/about/system/managed/latex/doc/endfloat.pdf
#   - \usepackage{setspace}\singlespacing
#   - \usepackage{lineno}
#   - \linenumbers
#   - \usepackage{tabu}
#   - \usepackage{pdflscape}
#   # - \usepackage{lscape}
#   - \newcommand{\blandscape}{\begin{landscape}}
#   - \newcommand{\elandscape}{\end{landscape}}

---

```{r setup, include = FALSE}
library("papaja")
r_refs("NAS_response.bib")
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE)
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(egg)
library(magick)
```

```{r GlobalOptions, include=FALSE}
options(knitr.duplicate.label = 'allow')
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
# knitr::opts_chunk$set(fig.pos = 'h')
```
\renewcommand{\arraystretch}{1}
# Data Review and Validation

Below I present a brief overview of the methods used to review the contents of 5 data sets used by Goad and Chartwell to visualize trends in DEI-language use. These data sets can be found in the `'out/twitter'`,`'out/grants'`, and `'out/scholarship'` folders of the NAS Report's Github repository [@goadIdeologicalIntensificationSTEM2023].

1. University Twitter accounts: `tweets_clean.csv`
1. National Science Foundation (i.e., NSF) grants: `nsf_all_grants_summary_data.csv` 
1. National Institutes of Health (i.e., NIH) grants: `nih_parsed_all.fst`
1. Scientific publications indexed in Google Scholar: `google_scholar.fst`
1. Scientific publications indexed in PubMed: `pubmed.fst`   

Although many of these errors would be detected immediately by simply scanning the data sets, I  wrote code in the R statistical programming language [@rcoreteamLanguageEnvironmentStatistical2020] to conduct some simple data validation tests. This code, which included functions from the `tidyverse` [@wickhamWelcomeTidyverse2019] and `janitor` [@firkeJanitorSimpleTools2021] libraries for filtering, de-duplicating, and summarizing data frames, is available at [@brunaCodeIdentifyingErrors2023], as are `.csv` files of the resulting output. Below I provide summaries and representative examples of the errors revealed by the validation procedures . 

## _University Twitter accounts_

```{r cached=TRUE, echo = FALSE, warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
twitter_notdei<-read_csv("./error_analysis_bruna/validation_output/twitter_notdei.csv",col_types = cols())
twitter_notdei_summary<-read_csv("./error_analysis_bruna/validation_output/twitter_notdei_summary.csv",col_types = cols())

total_tweets_reviewed<-twitter_notdei_summary %>% 
  select(total_tweets_reviewed) %>% 
  slice(1)

total_tweets_clean<-twitter_notdei_summary %>% 
  select(total_tweets_clean) %>% 
  slice(1)

total_tweets_fail<-twitter_notdei_summary %>% 
  summarize(sum(fail_tweets))

min_perc<-round(min(twitter_notdei_summary$perc_fail),2)
max_perc<-round(max(twitter_notdei_summary$perc_fail),2)
```
Goad and Chartwell searched 895 university accounts for over 20 terms they define as DEI-related [@goad2022ideological]. They used the resulting dataset of N = `r total_tweets_clean` tweets (`'tweets_clean.csv'`) to graph the use of the DEI-terms over time. Many of the terms for which they searched, however, have uses and meanings beyond DEI. For instance, "race" could refer to competitions or athletic events, "ally" is a common nickname for "Allison", "justice" is the title used by members of federal or state bench, and introductions are often prefaced by the phrase "it is my privilege to...".   
I reviewed Goad and Chartwell's twitter dataset for tweets that might be using seven of their DEI-related search terms in a non-DEI context. These terms were: "advocacy", "ally", "diversity", "equity", "justice", "privilege", and "race". I first filtered `'tweets_clean.csv'` for all tweets they assigned to a terms (e.g., "race"), then searched this subset of tweets for strings related to non-DEI uses of that term (e.g., "5K", "nascar", "sailing"). To ensure that the resulting tweets were not related to DEI, I eliminated any that included the entire suite of DEI-terms with which Goad and Chartwell conducted their searches (e.g., "racism", "equality", "gender", "social justice", "blm", "equity", see `"validation code"` in [@brunaCodeIdentifyingErrors2023]). Note that this method provides a conservative estimate of any non-DEI tweets that were included in Goad and Chartwell's analyses, as it will only capture tweets using the non-DEI terms for which I searched. The complete list of filtering strings for each of the 7 DEI-terms I reviewed can be found in `'twitter_errors.R'`; the file `'twitter_notdei.csv'` contains the collection of non-DEI tweets returned by this algorithm (See Table 1 for examples) is.   
  The seven search terms reviewed comprise N = `r total_tweets_reviewed` tweets, which is `r round(total_tweets_reviewed/total_tweets_clean*100,2)`% of Goad and Chartwell's twitter dataset. With the conservative validation method described above, I found that `r round(((total_tweets_fail/total_tweets_reviewed)*100),2)`% of the tweets for the seven focal terms  were not actually DEI-related, with the percentage of irrelevant tweets for a given term ranging from `r min_perc` - `r max_perc`% (Table 2). If there were no additional errors in these or the remaining 14 terms, the overall error rate for the entire data set would be `r round(((total_tweets_fail/total_tweets_clean)*100),2)`%.

## _NIH and NSF grants_ 

```{r cached=TRUE, echo = FALSE, warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
nsf_div_grants<-read_csv("./error_analysis_bruna/validation_output/grants_nsf_diversity.csv",col_types = cols())
grant_dupes<-read_csv("./error_analysis_bruna/validation_output/grants_dupes.csv",col_types = cols())
grant_summary<-read_csv("./error_analysis_bruna/validation_output/grant_dupes_summary.csv",col_types = cols())

number_nsfdiv<-nrow(nsf_div_grants)


```

I found two major sources of error in Goad and Chartwell's NSF and NIH data. First, their sample sizes for the number of grants were vastly inflated because they failed to correct for the mechanism by which these agencies transfer funds to the different institutions collaborating on a successful proposal. When a grant proposal that includes collaborators at different institutions is selected for funding, the agency will transfer each researcher's portion of the grant's budget directly to each institution.A single successful grant proposal may therefore be represented in the agency's database by multiple "awards". By not consolidating different awards for the same proposal in their dataset, Goad and Chartwell have inflated their estimates of the number of NSF and NIH grants in their dataset by `r grant_summary %>% filter(dataset=="NSF") %>% select(perc_inflated)`% and `r grant_summary %>% filter(dataset=="NIH") %>% select(perc_inflated)`%, respectively. The file `'grants_dupes.csv'` contains all duplicated grant records.   
  Goad and Chartwell also failed to screen for alternative uses of their focal terms when reviewing the NSF and NIH grants. For example, N = `r number_nsfdiv` of the NSF grants they identify as being DEI-focused when searching with the term "diversity" are actually grants for ecological or evolutionary research on genetic, phylogenetic, or species diversity (see `'grants_nsf_diversity.csv'`, see Table 3 for examples). 


## _Scientific publications in Google Scholar_

```{r cached=TRUE, echo = FALSE, warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
gs_dupes<-read_csv("./error_analysis_bruna/validation_output/gs_dupes.csv",col_types = cols())
gs_sources<-read_csv("./error_analysis_bruna/validation_output/gs_sources_all_summary.csv",col_types = cols())
gs_sources_not_stem<-read_csv("./error_analysis_bruna/validation_output/gs_sources_not_stem_summary.csv",col_types = cols())

pm_sources_not_stem<-read_csv("./error_analysis_bruna/validation_output/pm_sources_not_stem_summary.csv",col_types = cols())
pm_dupes<-read_csv("./error_analysis_bruna/validation_output/pm_dupes.csv",col_types = cols())
pm_sources<-read_csv("./error_analysis_bruna/validation_output/pm_sources_all_summary.csv",col_types = cols())
pm_nondei<-read_csv("./error_analysis_bruna/validation_output/pm_nondei_examples.csv",col_types = cols())
gs_nondei<-read_csv("./error_analysis_bruna/validation_output/gs_neurology_examples.csv",col_types = cols())
pubs_dupes_summary<-read_csv("./error_analysis_bruna/validation_output/pubs_dupes_summary.csv",col_types = cols())

# nrow(pm_nondei)+nrow(gs_nondei)
```
  Finally, Goad and Chartwell sought to identify DEI-related publications in the scientific literature. To do so they searched the repositories Google Scholar, arXiv, Web of Science, and PubMed for DEI-related articles in science, technology, engineering, and mathematics (STEM) journals by using search strings including a STEM-term and one of their DEI-related terms (e.g., "biology diversity"). I reviewed their data from Google Scholar and Pubmed.  
  Goad and Chartwell once again failed to search their results for duplicate records. The `r sum(pubs_dupes_summary$n_original)-sum(pubs_dupes_summary$n_deduped)` duplicates that remained in these datasets inflated their estimate of DEI-related publications in Google Scholar and PubMed by `r pubs_dupes_summary %>% filter(dataset=="gs") %>% select(perc_inflated)`% and `r pubs_dupes_summary %>% filter(dataset=="pubmed") %>% select(perc_inflated)`%.  They also failed to exclude hundreds of articles that were published in cultural studies, humanities, and legal journals (Table 4 and ), as well as thousands of non-DEI articles on topics ranging from palliative care for cancer patients to transcatheter aortic valve replacements (see Table 5, and `'gs_neurology_examples.csv'`, `'pm_nondei_examples.csv'`.  

## Conclusion

A review of the data used in Goad and Chartwell's NAS report finds it includes thousands of duplications and irrelevant records. It is important to emphasize that the error estimates presented are conservative, as the procedures described here are merely a "first pass" using relatively simple methods; more robust validation efforts, for example using keyword co-associations, will almost certainly identify additional errors. 

# Bibliography   


::: {#refs custom-style="Bibliography"}
:::
