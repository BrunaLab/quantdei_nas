---
title             : "Supplement to Bruna: Fundamental errors of data collection & validation undermine claims of 'Ideological Intensification' made by the National Association of Scholars"
shorttitle        : "Flawed data validation by the NAS"

author: 
  - name          : "Emilio M. Bruna"
    affiliation   : "1,2"
#    address       : "Department of Wildlife Ecology and Conservation, University of Florida, PO Box 110430, Gainesville, FL 32611-0430, USA"
    email         : "embruna@ufl.edu"
    # role:
    #   - Methodology
    #   - Data curation
    #   - Investigation
    #   - Funding acquisition
    #   - Conceptualization
    #   - Formal analysis
    #   - Methodology
    #   - Project administration
    #   - Resources
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Visualization
    #   - Writing original draft

affiliation:
  - id            : "1"
    institution   : "Department of Wildlife Ecology and Conservation, University of Florida, PO Box 110430, Gainesville, FL 32611-0430, USA"
  - id            : "2"
    institution   : "Center for Latin American Studies, University of Florida, PO Box 115530, Gainesville, FL 32611-5530, USA"

authornote: All code and data used in this analysis are available at https://github.com/embruna/quantdei_nas. 

# 
# abstract: |
#   Abstract of the paper.
  
  
  
keywords          : "Up,to,eight,keywords"
wordcount         : "X"

bibliography      : "NAS_response.bib"

floatsintext      : no
figurelist        : yes
tablelist         : no
footnotelist      : no
numbersections    : no
linenumbers       : yes
mask              : no
draft             : no
# replace_ampersands: yes

classoption       : "man, donotrepeattitle" # suppresses title on 1st page of MS
output            : papaja::apa6_pdf
mainfont: Times New Roman
fontsize: 12pt
csl: "science.csl"
fig_caption: yes
keep_tex: yes

header-includes:
  - \raggedbottom
  - \usepackage{endfloat} #[nomarkers] excludes the {insert figure x around here] from main text. The others exclude the list of tables and figures. https://cs.brown.edu/about/system/managed/latex/doc/endfloat.pdf
  - \usepackage{setspace}\doublespacing
  - \usepackage{lineno}
  - \linenumbers
  - \usepackage{tabu}
  - \usepackage{pdflscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}

---


```{r setup, include = FALSE}
library("papaja")
r_refs("NAS_response.bib")
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE)
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(egg)
library(magick)
```

```{r GlobalOptions, include=FALSE}
options(knitr.duplicate.label = 'allow')
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
# knitr::opts_chunk$set(fig.pos = 'h')
```

# Data Review and Validation

Below I present a brief overview of the methods used to review the contents of four of `'clean'` data sets used by Goad and Chartwell to visualize trends in DEI-language use: 

1. University Twitter accounts: `tweets_clean.csv`
1. Grants awarded by the National Science Foundation (i.e., NSF):  `nsf_all_grants_summary_data.csv`  
1. Grants awarded by the National Institutes of Health (i.e., NIH): `nih_parsed_all.fst`  
1. Scientific publications indexed in Google Scholar: `google_scholar.fst` 

These data sets can all be found in the `'out'` folder of the Github repository hosting the code used for the NAS report [@scholarsQuantitativeStudyDiversity2022].  
To review their datasets I used code written in the R statistical programming language [@rcoreteamLanguageEnvironmentStatistical2020] with functions from the `tidyverse` [@wickhamWelcomeTidyverse2019] and `janitor` [@firkeJanitorSimpleTools2021] libraries for filtering, de-duplicating, and summarizing data frames. This code and the resulting output are available at [@BrunaLabQuantdeiNas]. Below I provide summaries and representative examples of the errors revealed by the validation procedures; it is important to emphasize that any error estimates presented are conservative, as this was merely a "first pass" using simple methods. More robust validation efforts will almost certainly identify additional errors. 

## _University Twitter accounts_

```{r cached=TRUE, echo = FALSE, warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
twitter_notdei<-read_csv("./error_analysis_bruna/validation_output/twitter_notdei.csv",col_types = cols())
twitter_notdei_summary<-read_csv("./error_analysis_bruna/validation_output/twitter_notdei_summary.csv",col_types = cols())

total_tweets_reviewed<-twitter_notdei_summary %>% 
  select(total_tweets_reviewed) %>% 
  slice(1)

total_tweets_clean<-twitter_notdei_summary %>% 
  select(total_tweets_clean) %>% 
  slice(1)

total_tweets_fail<-twitter_notdei_summary %>% 
  summarize(sum(fail_tweets))

min_perc<-round(min(twitter_notdei_summary$perc_fail),2)
max_perc<-round(max(twitter_notdei_summary$perc_fail),2)
```

Goad and Chartwell searched 895 university accounts for 21 terms they define as DEI-related. They used the resulting dataset of N = `r total_tweets_clean` tweets (`'tweets_clean.csv'`) to graph the use of the DEI-terms over time. Many of the terms for which they searched, however, have uses and meanings beyond DEI. For instance, "race" could refer to competitions or athletic events, "ally" is a common nickname for "Allison", and introductions are often prefaced by the phrase "it is my privilege to...".   
I reviewed Goad and Chartwell's twitter dataset for tweets that might be using seven of their DEI-related search terms in a non-DEI context. These terms were: "advocacy", "ally", "diversity", "equity", "justice", "privilege", and "race". I first filtered `'tweets_clean.csv'` for all tweets they assigned to a terms (e.g., "race"), then searched this subset of tweets for strings related to non-DEI uses of that term (e.g., "5K", "nascar", "sailing"). To ensure that the resulting tweets were not related to DEI, I eliminated any that included the other  DEI-terms for which they searched (e.g., "racism", "equality", "gender", "social justice)". I then reviewed the final list of tweets for each term manually. Note that this method provides a conservative estimate of non-DEI tweets attributed by Goad and Chartwell to each term, as it will only capture tweets with the non-DEI search strings I included.  The complete list of filtering strings for each of the 7 DEI-terms I reviewed is in `'twitter_errors.R'`; the complete collection of non-DEI tweets is in `'twitter_notdei.csv'`.   
The seven search terms reviewed comprise `r round(total_tweets_reviewed/total_tweets_clean*100,2)`% of Goad and Chartwell's twitter dataset (N = `r total_tweets_reviewed` tweets). Based on the  conservative validation method described above, at least `r round(((total_tweets_fail/total_tweets_reviewed)*100),2)`% of these are not DEI-related, with the percentage of irrelevant tweets for a given term ranging from `r min_perc` - `r max_perc`%. If there were no additional errors in these or the remaining 14 terms, the overall error rate would be `r round(((total_tweets_fail/total_tweets_clean)*100),2)`%.

## _NIH and NSF grants_ 

```{r cached=TRUE, echo = FALSE, warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
nsf_div_grants<-read_csv("./error_analysis_bruna/validation_output/grants_nsf_diversity.csv",col_types = cols())
grant_dupes<-read_csv("./error_analysis_bruna/validation_output/grants_dupes.csv",col_types = cols())


number_nsfdiv<-nrow(nsf_div_grants)

perc_duped_nsf<-grant_dupes %>% filter(agency=="nsf") %>% select(percent_duplicated_rows) 
perc_duped_nih<-grant_dupes %>% filter(agency=="nih") %>% select(percent_duplicated_rows) 
```

Goad and Chartwell also failed to screen for alternative uses of their focal terms when reviewing the grants awarded by NSF and NIH. For example, N = `r number_nsfdiv` of the NSF grants they identify as being DEI-focused when searching with the term "diversity" are actually grants for ecological or evolutionary research on genetic, phylogenetic, or species diversity (see `'grants_nsf_diversity.csv'`). That aside, they also inflated their estimate for the total number of DEI-grants awarded because they failed to account for how NSF and NIH transfer funds to grant collaborators. A single proposal funded by these agencies will often be represented in the award databases by multiple "award" records because the researchers based at different institutions will be allocated their respective portions of the grant as separate awards. While calculating the total support by NSF and NIH of DEI-related activities requires tabulating the amount of individual awards, failing to consolidate the awards related to the same project, they inflated the total number of NSF and NIH grants by `r perc_duped_nsf`% and `r perc_duped_nih`%, respectively. 
 

These duplicatons inflated the number of NSF and NIH grants by at least `r perc_duped_nsf`% and `r perc_duped_nih`%, respectively.

## _Scientific publications in Google Scholar_

```{r cached=TRUE, echo = FALSE, warning=FALSE,message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
gs_dupes<-read_csv("./error_analysis_bruna/validation_output/gs_dupes.csv",col_types = cols())
gs_sources<-read_csv("./error_analysis_bruna/validation_output/gs_sources_all_summary.csv",col_types = cols())
gs_dupes_summary<-read_csv("./error_analysis_bruna/validation_output/gs_dupes_summary.csv",col_types = cols())

number_gs_dupes<-nrow(gs_dupes)

```

Finally, Goad and Chartwell sought to identify any DEI-related publications in the scientific literature. To do so they searched a number of repositories, including Google Scholar, for DEI-related articles in science, technology, engineering, and mathematics (STEM) journals by using search strings including a STEM-term and one of their DEI-related terms (e.g., "biology diversity"). Here again they failed to search their results for duplicate or irrelevant records prior to graphing their results - over `r number_gs_dupes` of the records in their data set were duplicates (`r round(number_gs_dupes/(gs_dupes_summary$total_gs)*100,2)`%), and at least XXXX of the articles they considered DEI-related were incorrectly included. Moreover, their data set of 'DEI-related articles in STEM journals' included at least N = `r sum(gs_sources$n)` articles in humanities, cultural studies, and law journals. A partial list of these journals can be found in Table SX.     

\newpage




# References

::: {#refs custom-style="Bibliography"}
:::



```{r FigS1, echo=FALSE, message = FALSE, warning=FALSE, fig.align='center', fig.cap="Percentage of irrelevant tweets atrtibuted to seven different DEI search terms. Note that this percentage is a conservative estimate, as it is based on a preliminary review.", fig.fullwidth=TRUE,fig.height = 5,fig.width=5}
twitter_plot<-ggplot(data=twitter_notdei_summary, aes(x=reorder(category,perc_fail),y=perc_fail)) +
  geom_bar(stat="identity") +
  labs(x="DEI-related Terms",
       y="Irrelevant Tweets (%)",
       title=" ")+
  scale_y_continuous(breaks=c(0,5,10,15,20,25,30,35,40,45,50), expand = c(0, 0)) +
  theme_classic()+
  theme(
    # plot.title = element_text(face="bold", size=20,family = "Arial", colour = "black"),
        # Sets title size, style, location
        # legend.position=c(0.5,0.95),
        axis.line.y = element_line(color="black", size = 0.5, lineend="square"),
        axis.line.x = element_line(color="black", size = 0.5, lineend="square"),
        axis.title.x=element_text(colour="black", size = 14, vjust=-1,hjust=0.5),           #S ets x
        axis.title.y=element_text(colour="black", size = 14, vjust=1.5,hjust=0.5),            #S ets y
        axis.text.x=element_text(colour="black", size = 10, angle = 0, vjust =0, hjust=0.5),
        axis.text.y=element_text(colour="black", size = 10, angle = 0, vjust =0, hjust=0),                          # Sets size and style of labels on axes
        # legend.title = element_blank(),                                             # Removes the Legend title
        # legend.key = element_blank(),                                              #R emoves the boxes around legend colors
        # legend.text = element_text(face="italic", color="black", size=12),
        # legend.position = "top",
        # legend.direction = 'horizontal',
        # legend.key = element_rect(colour = "black"),                              #s ets size and style of labels on axes
        # plot.margin = unit(c(1,2,1,1), "cm")
        plot.background = element_rect(fill = "white"),
        panel.background = element_rect(fill = "white")
  )
twitter_plot
```




```{r Table1, echo=FALSE,message = FALSE,warning=FALSE}

Table1<-twitter_notdei_summary %>% 
  mutate(perc_fail=round(perc_fail,2)) %>% 
  select(-total_tweets_reviewed,-total_tweets_clean)

names(Table1)<-c("DEI Term","Irrelevant Tweets (N)","Total Tweets (N)","% Irrelevant")

Table1<-knitr::kable(Table1, 
      digits = 2,
      align="lccc",
      format="latex",
      row.names = FALSE,
      booktabs=T,
      longtable=T,
      linesep = "", #removes the blank line after every 5 lines
      caption = "Irrelevant tweets attributed to seven different DEI terms and the total number of tweets for each term in the original dataset. Note that this percentage is a conservative estimate, as it is based on a preliminary review.") %>%
     kable_styling(bootstrap_options = c("hover"),
                 full_width = F,
                 latex_options="scale_down",
                 font_size = 12,
                position = "left")  


Table1
```






<!-- \blandscape -->
<!-- \newpage -->
```{r Table2, echo=FALSE,message = FALSE,warning=FALSE}

Table2<-twitter_notdei %>% 
  group_by(category) %>% 
  slice(1:3) %>% 
  rename("tweet"="text") %>% 
  select(category,tweet) %>% 
  mutate(tweet=gsub("[[:punct:]]", "", tweet))  %>% 
 mutate(tweet=str_sub(tweet, 1, 70))

# %>% 
#   mutate(text=gsub("\\\\", " ", tweet)
#                    
# Table1<-Tables[2]
# Table1<-as.data.frame(Table1)
# Table1<-arrange(Table1,desc(`APC..US..`))


names(Table2)<-c("DEI Term","sample irrelevant tweet")

Table2<-knitr::kable(Table2, 
      digits = 2,
      align="ll",
      format="latex",
      row.names = FALSE,
      booktabs=T,
      longtable=T,
      linesep = "", #removes the blank line after every 5 lines
      caption = "Sample tweets incorrectly attributed to different DEI terms.") %>%
  kable_styling(bootstrap_options = c("hover"),
                full_width = F,
                latex_options="scale_down",
                font_size = 12,
               position = "left") 

# %>%
#   footnote(number = c("OA Mirror title: Biochimie Open", "OA Mirror title: Micro and Nano Engineering")) %>% add_header_above(c(" " = 1, "Subscription Journal" = 2, "Mirror Journal" = 1))
# tmp<-str_replace(tmp, "Super1", "$^{1}$")
# tmp<-str_replace(tmp, "Super2", "$^{2}$")
# knitr::asis_output(tmp)
Table2
```


<!-- \elandscape -->